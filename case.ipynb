{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "case.ipynb",
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNLGhuxLNaYHZyZFrJ1VBmJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexWortega/Collab_experement/blob/main/case.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQVgEITzqWy4"
      },
      "source": [
        "# SNLI case solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVKPykQnVE1L"
      },
      "source": [
        "!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUyydMN2VNud"
      },
      "source": [
        "!unzip snli_1.0.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQB5Hq14i09U"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y93FJGp6WVp2"
      },
      "source": [
        "cd snli_1.0/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb4InenBWbRb"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUlxzMsbWkXf"
      },
      "source": [
        "\n",
        "df = pd.read_json(\"snli_1.0_dev.jsonl\",lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOJEvK_2XyAY"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oHpyuCjXzfG"
      },
      "source": [
        "labels =df['annotator_labels']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTtimrouYg0k"
      },
      "source": [
        "set(state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it2TNl9Mep07"
      },
      "source": [
        "! mkdir kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeUw6rkwemrN"
      },
      "source": [
        "!kaggle datasets download -d takuok/glove840b300dtxt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AO50g7Qes-7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3O8ahVZXMjv"
      },
      "source": [
        "from __future__ import print_function\n",
        "from functools import reduce\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import tarfile\n",
        "import tempfile\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "\n",
        "'''\n",
        "\n",
        "'''\n",
        "\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import merge, recurrent, Dense, Input, Dropout, TimeDistributed\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.regularizers import l2\n",
        "from keras.utils import np_utils\n",
        "\n",
        "def extract_tokens_from_binary_parse(parse):\n",
        "    return parse.replace('(', ' ').replace(')', ' ').replace('-LRB-', '(').replace('-RRB-', ')').split()\n",
        "\n",
        "def yield_examples(fn, skip_no_majority=True, limit=None):\n",
        "  for i, line in enumerate(open(fn)):\n",
        "    if limit and i > limit:\n",
        "      break\n",
        "    data = json.loads(line)\n",
        "    label = data['gold_label']\n",
        "    s1 = ' '.join(extract_tokens_from_binary_parse(data['sentence1_binary_parse']))\n",
        "    s2 = ' '.join(extract_tokens_from_binary_parse(data['sentence2_binary_parse']))\n",
        "    if skip_no_majority and label == '-':\n",
        "      continue\n",
        "    yield (label, s1, s2)\n",
        "\n",
        "def get_data(fn, limit=None):\n",
        "  raw_data = list(yield_examples(fn=fn, limit=limit))\n",
        "  left = [s1 for _, s1, s2 in raw_data]\n",
        "  right = [s2 for _, s1, s2 in raw_data]\n",
        "  print(max(len(x.split()) for x in left))\n",
        "  print(max(len(x.split()) for x in right))\n",
        "\n",
        "  LABELS = {'contradiction': 0, 'neutral': 1, 'entailment': 2}\n",
        "  Y = np.array([LABELS[l] for l, s1, s2 in raw_data])\n",
        "  Y = np_utils.to_categorical(Y, len(LABELS))\n",
        "\n",
        "  return left, right, Y\n",
        "\n",
        "training = get_data('/content/snli_1.0/snli_1.0_train.jsonl')\n",
        "validation = get_data('/content/snli_1.0/snli_1.0_dev.jsonl')\n",
        "test = get_data('/content/snli_1.0/snli_1.0_test.jsonl')\n",
        "\n",
        "tokenizer = Tokenizer(lower=False, filters='')\n",
        "tokenizer.fit_on_texts(training[0] + training[1])\n",
        "\n",
        "# Lowest index from the tokenizer is 1 - we need to include 0 in our vocab count\n",
        "VOCAB = len(tokenizer.word_counts) + 1\n",
        "LABELS = {'contradiction': 0, 'neutral': 1, 'entailment': 2}\n",
        "#RNN = recurrent.LSTM\n",
        "#RNN = lambda *args, **kwargs: Bidirectional(recurrent.LSTM(*args, **kwargs))\n",
        "#RNN = recurrent.GRU\n",
        "#RNN = lambda *args, **kwargs: Bidirectional(recurrent.GRU(*args, **kwargs))\n",
        "# Summation of word embeddings\n",
        "RNN = None\n",
        "LAYERS = 1\n",
        "USE_GLOVE = True\n",
        "TRAIN_EMBED = False\n",
        "EMBED_HIDDEN_SIZE = 300\n",
        "SENT_HIDDEN_SIZE = 300\n",
        "BATCH_SIZE = 512\n",
        "PATIENCE = 4 # 8\n",
        "MAX_EPOCHS = 42\n",
        "MAX_LEN = 42\n",
        "DP = 0.2\n",
        "L2 = 4e-6\n",
        "ACTIVATION = 'relu'\n",
        "OPTIMIZER = 'rmsprop'\n",
        "print('RNN / Embed / Sent = {}, {}, {}'.format(RNN, EMBED_HIDDEN_SIZE, SENT_HIDDEN_SIZE))\n",
        "print('GloVe / Trainable Word Embeddings = {}, {}'.format(USE_GLOVE, TRAIN_EMBED))\n",
        "\n",
        "to_seq = lambda X: pad_sequences(tokenizer.texts_to_sequences(X), maxlen=MAX_LEN)\n",
        "prepare_data = lambda data: (to_seq(data[0]), to_seq(data[1]), data[2])\n",
        "\n",
        "training = prepare_data(training)\n",
        "validation = prepare_data(validation)\n",
        "test = prepare_data(test)\n",
        "\n",
        "print('Build model...')\n",
        "print('Vocab size =', VOCAB)\n",
        "\n",
        "GLOVE_STORE = 'precomputed_glove.weights'\n",
        "if USE_GLOVE:\n",
        "  if not os.path.exists(GLOVE_STORE + '.npy'):\n",
        "    print('Computing GloVe')\n",
        "  \n",
        "    embeddings_index = {}\n",
        "    f = open('/content/drive/MyDrive/Colab Notebooks/Data/glove.840B.300d.txt')\n",
        "    for line in f:\n",
        "      values = line.split(' ')\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "    \n",
        "    # prepare embedding matrix\n",
        "    embedding_matrix = np.zeros((VOCAB, EMBED_HIDDEN_SIZE))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "      else:\n",
        "        print('Missing from GloVe: {}'.format(word))\n",
        "  \n",
        "    np.save(GLOVE_STORE, embedding_matrix)\n",
        "\n",
        "  print('Loading GloVe')\n",
        "  embedding_matrix = np.load(GLOVE_STORE + '.npy')\n",
        "\n",
        "  print('Total number of null word embeddings:')\n",
        "  print(np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
        "\n",
        "  embed = Embedding(VOCAB, EMBED_HIDDEN_SIZE, weights=[embedding_matrix], input_length=MAX_LEN, trainable=TRAIN_EMBED)\n",
        "else:\n",
        "  embed = Embedding(VOCAB, EMBED_HIDDEN_SIZE, input_length=MAX_LEN)\n",
        "\n",
        "rnn_kwargs = dict(output_dim=SENT_HIDDEN_SIZE, dropout_W=DP, dropout_U=DP)\n",
        "SumEmbeddings = keras.layers.core.Lambda(lambda x: K.sum(x, axis=1), output_shape=(SENT_HIDDEN_SIZE, ))\n",
        "\n",
        "translate = TimeDistributed(Dense(SENT_HIDDEN_SIZE, activation=ACTIVATION))\n",
        "\n",
        "premise = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "hypothesis = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "\n",
        "prem = embed(premise)\n",
        "hypo = embed(hypothesis)\n",
        "\n",
        "prem = translate(prem)\n",
        "hypo = translate(hypo)\n",
        "\n",
        "if RNN and LAYERS > 1:\n",
        "  for l in range(LAYERS - 1):\n",
        "    rnn = RNN(return_sequences=True, **rnn_kwargs)\n",
        "    prem = BatchNormalization()(rnn(prem))\n",
        "    hypo = BatchNormalization()(rnn(hypo))\n",
        "rnn = SumEmbeddings if not RNN else RNN(return_sequences=False, **rnn_kwargs)\n",
        "prem = rnn(prem)\n",
        "hypo = rnn(hypo)\n",
        "prem = BatchNormalization()(prem)\n",
        "hypo = BatchNormalization()(hypo)\n",
        "\n",
        "joint = merge([prem, hypo], mode='concat')\n",
        "joint = Dropout(DP)(joint)\n",
        "for i in range(3):\n",
        "  joint = Dense(2 * SENT_HIDDEN_SIZE, activation=ACTIVATION, W_regularizer=l2(L2) if L2 else None)(joint)\n",
        "  joint = Dropout(DP)(joint)\n",
        "  joint = BatchNormalization()(joint)\n",
        "\n",
        "pred = Dense(len(LABELS), activation='softmax')(joint)\n",
        "\n",
        "model = Model(input=[premise, hypothesis], output=pred)\n",
        "model.compile(optimizer=OPTIMIZER, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "print('Training')\n",
        "_, tmpfn = tempfile.mkstemp()\n",
        "# Save the best model during validation and bail out of training early if we're not improving\n",
        "callbacks = [EarlyStopping(patience=PATIENCE), ModelCheckpoint(tmpfn, save_best_only=True, save_weights_only=True)]\n",
        "model.fit([training[0], training[1]], training[2], batch_size=BATCH_SIZE, nb_epoch=MAX_EPOCHS, validation_data=([validation[0], validation[1]], validation[2]), callbacks=callbacks)\n",
        "\n",
        "# Restore the best found model during validation\n",
        "model.load_weights(tmpfn)\n",
        "\n",
        "loss, acc = model.evaluate([test[0], test[1]], test[2], batch_size=BATCH_SIZE)\n",
        "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}