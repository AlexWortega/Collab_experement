{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Paper_summary.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "C9qehmLWJcZ9"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexWortega/Collab_experement/blob/main/Paper_summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvkkKT4qVfM2"
      },
      "source": [
        "#Research paper summary\r\n",
        "* Запустите последовательно первые две ячейки\r\n",
        "* Замените None на линку на .pdf файл \r\n",
        "* По идее все должно работать\r\n",
        "-------\r\n",
        "##todo\r\n",
        "* парсить формулы и выводить отдельно\r\n",
        "* вывод в виде .pdf\r\n",
        "* узнать почему пдф парсер выдает какие то ключи "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9qehmLWJcZ9"
      },
      "source": [
        "#Инсталятор"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXu_yLgiJJDk",
        "outputId": "b2f2ad9e-3c13-4f06-d5aa-f37a4c8847e0"
      },
      "source": [
        "pip install bert-extractive-summarizer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-extractive-summarizer in /usr/local/lib/python3.6/dist-packages (0.6.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from bert-extractive-summarizer) (2.1.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from bert-extractive-summarizer) (0.22.2.post1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (from bert-extractive-summarizer) (2.2.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (0.8.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (1.19.4)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (2.6.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (2.23.0)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (0.9.6)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (0.2.4)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (7.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (2.0.5)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (2.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (0.0.43)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (1.16.51)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (0.1.95)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (7.1.2)\n",
            "Requirement already satisfied: botocore<1.20.0,>=1.19.51 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers->bert-extractive-summarizer) (1.19.51)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers->bert-extractive-summarizer) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers->bert-extractive-summarizer) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.51->boto3->transformers->bert-extractive-summarizer) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQSU2LB2Jber",
        "outputId": "4dd73c82-7767-48c7-8d53-5e9bc260c98f"
      },
      "source": [
        "pip install spacy==2.1.3"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy==2.1.3 in /usr/local/lib/python3.6/dist-packages (2.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (1.0.5)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (0.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (1.0.5)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (2.6.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (2.0.5)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (0.9.6)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (2.0.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (0.8.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (2.23.0)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (7.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (1.19.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3) (3.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.2->spacy==2.1.3) (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkYfMht0JmVx",
        "outputId": "bd2248ee-7ac4-45d3-e474-7a2d65bf5eba"
      },
      "source": [
        "pip install transformers==2.2.2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==2.2.2 in /usr/local/lib/python3.6/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.2.2) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.2.2) (0.1.95)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.2.2) (1.16.51)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers==2.2.2) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.2.2) (1.19.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers==2.2.2) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.2.2) (0.0.43)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.2.2) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.2.2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.2.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.2.2) (3.0.4)\n",
            "Requirement already satisfied: botocore<1.20.0,>=1.19.51 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.2.2) (1.19.51)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.2.2) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.2.2) (0.10.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.2.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.2.2) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.2.2) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.51->boto3->transformers==2.2.2) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_UUMU7BJpNv",
        "outputId": "5e7da320-f966-4b21-96f3-f150d8e08ac1"
      },
      "source": [
        "pip install neuralcoref"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: neuralcoref in /usr/local/lib/python3.6/dist-packages (4.0)\n",
            "Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (2.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (1.19.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (1.16.51)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.1)\n",
            "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (2.6.0)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (0.9.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.5)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (7.0.8)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (0.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (0.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2020.12.5)\n",
            "Requirement already satisfied: botocore<1.20.0,>=1.19.51 in /usr/local/lib/python3.6/dist-packages (from boto3->neuralcoref) (1.19.51)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->neuralcoref) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->neuralcoref) (0.10.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.2->spacy>=2.1.0->neuralcoref) (4.41.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.51->boto3->neuralcoref) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.51->boto3->neuralcoref) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjuKXrc4RVgh",
        "outputId": "bb6ae3cf-2922-4935-826d-4940f7e16c0e"
      },
      "source": [
        "pip install nameparser"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nameparser in /usr/local/lib/python3.6/dist-packages (1.0.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ats6g6SZJsTW",
        "outputId": "15069221-9cf4-4df4-e94b-ba696ff4b847"
      },
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_md==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz#egg=en_core_web_md==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKi5hz_PJ4aj",
        "outputId": "dfabf521-d863-4fef-c030-20bef258607d"
      },
      "source": [
        "pip install wget"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXb46ekELALW",
        "outputId": "8eac881d-9b95-4fdf-db62-5af94e37ef5f"
      },
      "source": [
        "pip install tika"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tika in /usr/local/lib/python3.6/dist-packages (1.24)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from tika) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tika) (51.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6SeLstGNMkX",
        "outputId": "6f0e6d9b-0195-4a30-a5bd-d1f7f2415dc3"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "nltk.download('maxent_ne_chunker')\r\n",
        "nltk.download('words')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeII7tchJbnp"
      },
      "source": [
        "#тут логика\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBcESM4-Kfv9"
      },
      "source": [
        "import wget\r\n",
        "import nltk\r\n",
        "from nameparser.parser import HumanName\r\n",
        "from nltk.corpus import wordnet\r\n",
        "from summarizer import Summarizer\r\n",
        "import re\r\n",
        "\r\n",
        "filer = None\r\n",
        "def file_loader(url):\r\n",
        "    try:\r\n",
        "      wget.download(url, 'paper.pdf')\r\n",
        "      print('succsessfuly download .pdf from '+url)\r\n",
        "    except:\r\n",
        "      print('something wents wrong')\r\n",
        "\r\n",
        "\r\n",
        "def urlsfrompaper(text):\r\n",
        "    \r\n",
        "    return re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\r\n",
        "\r\n",
        "\r\n",
        "person_list = []\r\n",
        "person_names=person_list\r\n",
        "def get_human_names(text):\r\n",
        "    tokens = nltk.tokenize.word_tokenize(text)\r\n",
        "    pos = nltk.pos_tag(tokens)\r\n",
        "    sentt = nltk.ne_chunk(pos, binary = False)\r\n",
        "\r\n",
        "    person = []\r\n",
        "    name = \"\"\r\n",
        "    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\r\n",
        "        for leaf in subtree.leaves():\r\n",
        "            person.append(leaf[0])\r\n",
        "        if len(person) > 1: #avoid grabbing lone surnames\r\n",
        "            for part in person:\r\n",
        "                name += part + ' '\r\n",
        "            if name[:-1] not in person_list:\r\n",
        "                person_list.append(name[:-1])\r\n",
        "            name = ''\r\n",
        "        person = []\r\n",
        "    return (person_list)\r\n",
        "\r\n",
        "\r\n",
        "def cleaner(text):\r\n",
        "  names = get_human_names(text)+urlsfrompaper(text)+ ['pages','arxiv','2016','2017','2018']\r\n",
        "  for name in names:\r\n",
        "    text = text.replace(name,'')\r\n",
        "  return text\r\n",
        "\r\n",
        "def summarizer(main_text):\r\n",
        "  \r\n",
        "  model = Summarizer()\r\n",
        "  result = model(main_text, min_length=100)\r\n",
        "  full = ''.join(result)\r\n",
        "  return full\r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRb32XC1T9KG"
      },
      "source": [
        "#Вставте линку и запустите ячейку ниже"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEi5f5HLO0Sf",
        "outputId": "213217b1-75d1-4a27-fdf6-962e94ff4c7a"
      },
      "source": [
        "#@title String fields\r\n",
        "\r\n",
        "link = 'https://arxiv.org/pdf/2008.08995.pdf' #@param {type:\"string\"}\r\n",
        "\r\n",
        "from tika import parser\r\n",
        "import glob\r\n",
        "import os\r\n",
        "\r\n",
        "url = 'https://arxiv.org/pdf/2008.08995.pdf'\r\n",
        "file_loader(link)\r\n",
        "\r\n",
        "raw = parser.from_file('paper.pdf')\r\n",
        "main_text = cleaner(raw['content'])\r\n",
        "\r\n",
        "print(summarizer(main_text))\r\n",
        "print('________________________________________________________')\r\n",
        "print(urlsfrompaper(main_text))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "succsessfuly download .pdf from https://arxiv.org/pdf/2008.08995.pdf\n",
            "08\n",
            "99\n",
            "\n",
            "5v\n",
            "1 \n",
            "\n",
            " [\n",
            "cs\n",
            "\n",
            ".C\n",
            "L\n",
            "\n",
            "] \n",
            " 2\n",
            "\n",
            "0 \n",
            "A\n",
            "\n",
            "ug\n",
            " 2\n",
            "\n",
            "02\n",
            "0\n",
            "\n",
            "Constructing a Knowledge Graph\n",
            "from Unstructured Documents without External Alignment\n",
            "\n",
            "Seunghak Yu, Tianxing He, and \n",
            "\n",
            "MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA, USA\n",
            "\n",
            "{seunghak,cloudygoose,glass}@csail.mit.edu\n",
            "\n",
            "Abstract\n",
            "\n",
            "Knowledge graphs (KGs) are relevant to many\n",
            "\n",
            "NLP tasks, but building a reliable domain-\n",
            "\n",
            "specific KG is time-consuming and expen-\n",
            "\n",
            "sive. To summarize our\n",
            "\n",
            "approach, we first extract knowledge tuples\n",
            "\n",
            "in their surface form from unstructured doc-\n",
            "\n",
            "uments, encode them using a pre-trained lan-\n",
            "\n",
            "guage model, and link the surface-entities via\n",
            "\n",
            "the encoding to form the graph structure. However, creating a human-annotated KG\n",
            "\n",
            "is expensive, typically requiring large amounts\n",
            "\n",
            "of expert labor. 2008), human knowledge is\n",
            "\n",
            "continually expanding so knowledge bases need\n",
            "\n",
            "continuous refinement or will tend to become\n",
            "\n",
            "outdated. In this work, we aim to build a system com-\n",
            "\n",
            "bining advantages from both worlds, by building\n",
            "\n",
            "a virtual KG from unstructured documents. 3) By structuring documents as\n",
            "\n",
            "a KG, we enable a mechanism for interpretability\n",
            "\n",
            "during multi-hop reasoning. Note that\n",
            "\n",
            "in the formulations we assume each document con-\n",
            "\n",
            "sists of M sentences just for notation convenience. 2.1 Building a Knowledge Graph from\n",
            "\n",
            "Unstructured Documents\n",
            "\n",
            "Creating a graph from raw text is at the core of\n",
            "\n",
            "this work. Surface-entity Linking: The third and final\n",
            "\n",
            "KG building step performs surface-entity linking,\n",
            "\n",
            "which creates a graph structure out of the extracted\n",
            "\n",
            "entity-relation triples in a document D. The goal\n",
            "\n",
            "is to link entities with the same underlying concept\n",
            "\n",
            "together, for example from Table 1, a new surface-\n",
            "\n",
            "entity “the film” could be referring to “the\n",
            "\n",
            "Goonies” in some follow-up sentence, so the re-\n",
            "\n",
            "lations with “the film” should also be applied\n",
            "\n",
            "to “the Goonies”. We denote the set of surface-entities linked to sei\n",
            "as Link(sei), which is formulated below:\n",
            "\n",
            "Link(sei) = {sej : cos(se\n",
            "enc\n",
            "i , se\n",
            "\n",
            "enc\n",
            "j ) ≥\n",
            "\n",
            "λ ∗ max\n",
            "l∈ED\n",
            "\n",
            "(cos(seenci , se\n",
            "enc\n",
            "l ))}\n",
            "\n",
            "(1)\n",
            "\n",
            "Note that ED denotes the set of all surface-\n",
            "\n",
            "entities existing in document D. λ is a hyper-\n",
            "\n",
            "parameter controlling the adaptive threshold, and\n",
            "\n",
            "we found that a setting of 0.6 works well in our\n",
            "\n",
            "experiments. For each path p, we concatenate its surface-\n",
            "\n",
            "entities/relations with a period between each triple,\n",
            "\n",
            "and feed it to the BERT model to get an encoding\n",
            "\n",
            "of this path penc. N/A 68.30\n",
            "\n",
            "1-hop QA (WikiMovies)\n",
            "1 43.36 64.40 69.67\n",
            "2 45.75 56.67 60.95\n",
            "3 54.01 63.76 68.49\n",
            "\n",
            "Oracle N/A 55.20 66.76 71.03\n",
            "\n",
            "2-hop QA (MetaQA)\n",
            "1 21.14 34.56 37.79\n",
            "2 29.51 43.20 49.56\n",
            "3 35.26 44.95 49.71\n",
            "\n",
            "Oracle N/A 21.13 39.82 48.01\n",
            "\n",
            "3-hop QA (MetaQA)\n",
            "1 18.27 31.48 35.50\n",
            "2 20.91 27.67 34.07\n",
            "3 21.48 32.71 38.53\n",
            "\n",
            "Oracle N/A 22.01 43.34 59.37\n",
            "\n",
            "Table 2: Experimental results with changes in hops to\n",
            "\n",
            "navigate the graph. The test data consists\n",
            "\n",
            "of 9,952/14,872/14,274 QA pairs that require\n",
            "\n",
            "1-hop (WikiMovies), 2-hop, and 3-hop (MetaQA)\n",
            "\n",
            "inference, respectively. Although their performance\n",
            "\n",
            "cannot therefore be directly compared to our setup,\n",
            "\n",
            "we nevertheless use it as a baseline performance in-\n",
            "\n",
            "dicator. 4 Related Work\n",
            "\n",
            "Automatic knowledge graph construction: In\n",
            "\n",
            "most research to create knowledge graphs from\n",
            "\n",
            "unstructured text without human intervention, the\n",
            "\n",
            "popular approach is to develop a pipeline of NLP\n",
            "\n",
            "operations such as named entity recognition, en-\n",
            "\n",
            "tity linking and relationship extraction (Wu et al., 2019) builds a question-relevant\n",
            "\n",
            "sub-graph from the knowledge base or text cor-\n",
            "\n",
            "pus to gather all the relevant information. Acknowledgments\n",
            "\n",
            "Research was sponsored by the United States Air\n",
            "\n",
            "Force Research Laboratory and was accomplished\n",
            "\n",
            "under  Number FA8750-\n",
            "\n",
            "19-2-1000. In Proceedings of the 55th An-\n",
            "nual Meeting of the Association for Computational\n",
            "Linguistics (Volume 1: Long Papers),  1870–\n",
            "1879. In Proceedings of the 56th Annual Meeting of\n",
            "the Association for Computational Linguistics (Vol-\n",
            "ume 1: Long Papers),  845–855.\n",
            "________________________________________________________\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}